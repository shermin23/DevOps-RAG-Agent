# DevOps RAG Agent

A **Retrieval-Augmented Generation (RAG)** system for intelligent DevOps troubleshooting. Upload your documentation, paste error logs, and get AI-powered diagnostic solutions with traceable citations.

**Status:** Prototype ‚Üí Production Ready (scalable architecture included)

---

## üöÄ Quick Start (5 minutes)

### Prerequisites
- **Node.js** 18+ ([download](https://nodejs.org/))
- **Google AI API Key** ([free tier available](https://ai.google.dev/))
- **npm** or **yarn**

### Step 1: Clone & Install
```bash
git clone https://github.com/shermin23/DevOps-RAG-Agent.git
cd DevOps-RAG-Agent
npm install
```

### Step 2: Configure API Key
Create a `.env.local` file in the project root:
```bash
VITE_API_KEY=your_google_ai_api_key_here
```

**Get API Key:**
1. Visit [Google AI Studio](https://ai.google.dev/)
2. Click "Get API Key"
3. Copy the key and paste it above

### Step 3: Run Development Server
```bash
npm run dev
```

Open **http://localhost:5173** in your browser.

### Step 4: Upload Documentation
1. Click **Knowledge Base** (left panel)
2. Click the **+** button
3. Paste your DevOps docs (K8s guides, deployment configs, troubleshooting steps)
4. Click **Ingest & Index**

### Step 5: Analyze Error Logs
1. Paste an error log in the **middle textarea**
2. Click **Start RAG Pipeline**
3. View AI-generated solution with citations on the **right panel**

---

## üìä How It Works

### Two-Stage Pipeline

```
Raw Error Log (Noisy)
        ‚Üì
[Stage 1] LLM Cleans Query
        ‚Üì
Semantic Search Query
        ‚Üì
[Local] Vector Search in Knowledge Base
        ‚Üì
Top 3 Relevant Documents
        ‚Üì
[Stage 2] LLM Generates Solution with Citations
        ‚Üì
Traceable Answer [Source ID: doc-X]
```

**Example:**

| Stage | Input | Output |
|-------|-------|--------|
| **Raw Log** | `[2023-10-10 10:00:45] ERROR: Connection refused at 192.168.1.1 port 8080 (PID: 45821)` | |
| **LLM Clean** | ‚Üì | `Connection refused port 8080` |
| **Vector Search** | ‚Üì | Retrieved: mysql-config, connection-pooling, network-diagnostics |
| **LLM Generate** | ‚Üì | **Root Cause:** MySQL service unreachable. **Solution:** 1) Check `systemctl status mysql` 2) Verify firewall... [Source ID: mysql-config] |

---

## üèóÔ∏è Architecture

### Current (Development): Single-Page App (SPA)
```
Browser (React + Vite)
    ‚îú‚îÄ‚îÄ UI Components
    ‚îú‚îÄ‚îÄ Local Retrieval (keyword search)
    ‚îî‚îÄ‚îÄ State Management
              ‚Üì
    Google GenAI API (LLM calls)
```

**Pros:** No backend needed, runs offline for retrieval  
**Cons:** Limited scalability (100s docs), API keys exposed in browser

### Recommended (Production): Client-Server

See **[Deployment Guide](#-deployment-to-production)** below.

---

## üìÅ Project Structure

```
src/
‚îú‚îÄ‚îÄ App.tsx                    # Main orchestrator, state management
‚îú‚îÄ‚îÄ main.tsx                   # React entry point
‚îú‚îÄ‚îÄ types.ts                   # TypeScript interfaces
‚îú‚îÄ‚îÄ index.css                  # Global styles
‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îú‚îÄ‚îÄ DocManager.tsx         # Knowledge base upload/management
‚îÇ   ‚îî‚îÄ‚îÄ AnalysisDisplay.tsx    # Pipeline progress & results display
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îî‚îÄ‚îÄ gemini.ts              # Google GenAI wrapper (LLM calls)
‚îî‚îÄ‚îÄ utils/
    ‚îú‚îÄ‚îÄ search.ts              # Vector search (keyword matching)
    ‚îî‚îÄ‚îÄ textProcessor.ts       # Document chunking logic
```

---

## üîß Configuration

### Environment Variables

**Development (`.env.local`):**
```bash
VITE_API_KEY=sk-...           # Google AI API key
VITE_MAX_CHUNKS=5             # Top K results for retrieval
```

**Production (Docker/Server):**
```bash
API_KEY=sk-...                # API key (no VITE_ prefix in Node.js)
NODE_ENV=production
```

### Customize Behavior

**Change LLM Model** (`src/services/gemini.ts`):
```typescript
const model = "gemini-2.5-flash";  // Change to gemini-pro, gemini-pro-vision, etc.
```

**Change Search Results** (`src/App.tsx`):
```typescript
const retrievedChunks = retrieveRelevantChunks(searchQuery, allChunks, 5); // Adjust topK
```

**Change Document Chunk Size** (`src/utils/textProcessor.ts`):
```typescript
const CHUNK_SIZE = 500;  // Characters per chunk
```

---

## üåê Deployment to Production

### Option 1: Docker (Recommended for Teams)

#### 1a. Build Docker Image

Create `Dockerfile`:
```dockerfile
FROM node:20-alpine

WORKDIR /app

# Install dependencies
COPY package*.json ./
RUN npm ci --only=production

# Copy source & build
COPY . .
RUN npm run build

# Serve with Node.js static server
EXPOSE 3000
CMD ["npx", "serve", "-s", "dist", "-l", "3000"]
```

Build image:
```bash
docker build -t devops-rag-agent:latest .
```

#### 1b. Run Container Locally

```bash
docker run -e VITE_API_KEY=sk-xxxx -p 3000:3000 devops-rag-agent:latest
```

Open **http://localhost:3000**

#### 1c. Deploy to Cloud

**Vercel (Recommended):**
```bash
npm install -g vercel
vercel --env VITE_API_KEY=sk-xxxx
```

**AWS (EC2 + Docker):**
```bash
# SSH into EC2 instance
ssh -i your-key.pem ec2-user@your-instance-ip

# Pull image & run
docker run -d -e VITE_API_KEY=sk-xxxx -p 80:3000 \
  --name rag-app your-registry/devops-rag-agent:latest
```

**Google Cloud (Cloud Run):**
```bash
gcloud run deploy rag-agent \
  --image gcr.io/your-project/devops-rag-agent \
  --set-env-vars VITE_API_KEY=sk-xxxx \
  --platform managed
```

---

### Option 2: Add Backend API (For Vector Database & Scaling)

If you need to handle 1000s of documents, add a backend server:

**Install backend dependencies:**
```bash
npm install express cors dotenv
npm install -D @types/express
```

**Create `server.ts`:**
```typescript
import express from 'express';
import cors from 'cors';
import dotenv from 'dotenv';

dotenv.config();
const app = express();
app.use(cors());
app.use(express.json());

// API endpoint for retrieval
app.post('/api/search', async (req, res) => {
  const { query, docs } = req.body;
  
  // TODO: Replace keyword search with vector DB
  // Example: const results = await pinecone.query(embeddings);
  
  res.json({ chunks: [] });
});

// API endpoint for LLM generation
app.post('/api/generate', async (req, res) => {
  const { query, chunks } = req.body;
  
  const apiKey = process.env.API_KEY;
  if (!apiKey) throw new Error('API_KEY not set');
  
  // Call Gemini (keep API key server-side)
  res.json({ solution: '' });
});

app.listen(5000, () => console.log('Server on port 5000'));
```

**Update `App.tsx` to call backend:**
```typescript
const handleRunAnalysis = async () => {
  // Call backend instead of direct imports
  const response = await fetch('http://localhost:5000/api/generate', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ query: rawLog, chunks: docs })
  });
  const result = await response.json();
  setResult(result);
};
```

---

### Option 3: Serverless (AWS Lambda, Google Cloud Functions)

Deploy frontend to CDN + backend to serverless:

**Frontend:** Vercel, Netlify, AWS S3 + CloudFront  
**Backend:** AWS Lambda, Google Cloud Functions, Azure Functions

See [Serverless Architecture Guide](./docs/SERVERLESS.md) (create this file as needed).

---

## üß™ Development & Testing

### Run Development Server
```bash
npm run dev
```

### Run Type Checking
```bash
npm run lint
```

### Build for Production
```bash
npm run build
```

Preview build:
```bash
npm run preview
```

---

## üìö Key Features

- ‚úÖ **Two-Stage Query Cleaning** - Remove noise from error logs before retrieval
- ‚úÖ **Local Vector Search** - Keyword-based retrieval (no API calls needed)
- ‚úÖ **Citation Tracking** - Every LLM answer cites source documents [Source ID: X]
- ‚úÖ **Responsive UI** - Works on desktop, tablet, mobile
- ‚úÖ **Dark Theme** - Easy on the eyes for long troubleshooting sessions
- ‚úÖ **Type-Safe** - 100% TypeScript for safer code

### Planned Features
- üîÑ Real vector embeddings (OpenAI, Gemini Embedding API)
- üìä Real vector database (Pinecone, Weaviate)
- üîê User authentication & multi-tenant support
- üìà Analytics dashboard (query trends, success rates)
- üåç Multi-language support
- üîå Slack/PagerDuty integration

---

## ‚ö†Ô∏è Limitations & Known Issues

| Issue | Impact | Workaround |
|-------|--------|-----------|
| **Keyword-based search only** | Low accuracy on semantic queries | Upgrade to vector embeddings (Phase 2) |
| **API key in browser** (dev) | Security risk in production | Use backend API server |
| **No persistence** | Docs lost on page refresh | Add localStorage or database |
| **100 doc limit** | Retrieval becomes slow at scale | Move to real vector DB |
| **Sync-only processing** | UI blocks during LLM calls (2-3s) | Add background jobs / queues |

---

## üîê Security

### Development
- API key is exposed in browser (OK for local dev, **NOT for production**)

### Production
- Use backend server to keep API key server-side
- Implement authentication (OAuth, JWT)
- Use HTTPS only
- Add rate limiting on API endpoints
- Validate user inputs (prompt injection prevention)

**Example: Secure Backend Setup**
```typescript
// server.ts
const apiKey = process.env.API_KEY;  // Server-side only
const generateSolution = async (query: string) => {
  // User never sees API key
  return await geminiClient.generate({ query });
};
```

---

## ü§ù Contributing

### Report Issues
Found a bug? Open an [issue](https://github.com/shermin23/DevOps-RAG-Agent/issues)

### Suggest Features
Have ideas? Check [discussions](https://github.com/shermin23/DevOps-RAG-Agent/discussions)

### Submit Code
1. Fork the repo
2. Create a feature branch: `git checkout -b feature/your-feature`
3. Commit changes: `git commit -am 'Add feature'`
4. Push: `git push origin feature/your-feature`
5. Open a Pull Request

---

## üìñ Detailed Guides

- **[Architecture Deep Dive](./docs/ARCHITECTURE.md)** - How the RAG pipeline works
- **[API Reference](./docs/API.md)** - Service layer documentation
- **[Troubleshooting](./docs/TROUBLESHOOTING.md)** - Common issues & fixes
- **[Performance Tuning](./docs/PERFORMANCE.md)** - Optimize for scale

---

## üìû Support

- **Documentation:** Check [docs/](./docs/)
- **Issues:** [GitHub Issues](https://github.com/shermin23/DevOps-RAG-Agent/issues)
- **Email:** support@example.com

---

## üìÑ License

MIT License - see [LICENSE](./LICENSE) file for details

---

## üôã FAQ

**Q: Can I use this offline?**  
A: Yes, retrieval works offline. LLM calls require internet (Google API).

**Q: How much will it cost?**  
A: Gemini is free for development. Production: ~$0.075 per 1M tokens (~$1-5/month for small teams).

**Q: Can I replace Gemini with OpenAI / Claude / Llama?**  
A: Yes! Replace `src/services/gemini.ts` with your LLM provider. See [API Reference](./docs/API.md).

**Q: How do I scale to 10,000 documents?**  
A: Add a vector database (Pinecone, Weaviate) + backend API. See [Deployment Guide](#-deployment-to-production).

**Q: Is this suitable for production?**  
A: Yes, with these upgrades:
  1. Move API key to backend
  2. Add vector DB for better retrieval
  3. Implement authentication
  4. Set up monitoring & logging
  See [Production Checklist](./docs/PRODUCTION.md).

---

## üéØ Tech Stack

- **Frontend:** React 19 + TypeScript + Vite
- **Styling:** Tailwind CSS
- **LLM:** Google Gemini 2.5 Flash
- **Icons:** Lucide React
- **Markdown:** React Markdown
- **Build:** Vite (next-gen bundler)

---

## üìä Performance Metrics

| Metric | Value |
|--------|-------|
| **Dev startup** | <500ms (Vite HMR) |
| **LLM call latency** | 2-3 seconds |
| **Search latency** | <100ms (local) |
| **Total pipeline** | 6-8 seconds |
| **Bundle size** | ~200KB gzipped |
| **Max documents** | ~100 (keyword search) |

---

Made with ‚ù§Ô∏è by the DevOps community
